<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Visual Guide to Distance Measures in Data Science</title>
    <link rel="stylesheet" href="css/style.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/4.4.0/chart.min.js"></script>
</head>

<body>
    <div class="container">
        <a href="index.html" class="back-button">← Back to Blog</a>
        <div class="article-header-row">
            <div class="article-header-text">
                <h1>A Visual Guide to Distance Measures in Data Science</h1>
                <div class="article-meta">November 18, 2025 • 18 min read</div>
            </div>
            <img src="assets/images/distance-banner.png" alt="Distance Measures Banner" class="article-banner-side">
        </div>


        <div class="article-content">
            <p class="intro">
                Distance measures are the foundation of many machine learning algorithms. From clustering to
                classification,
                recommendation systems to anomaly detection—understanding how we measure similarity and dissimilarity
                between data points is crucial. In this visual guide, we'll explore the most important distance metrics
                through interactive animations.

            <p>The animations are created with Manim (Mathematical Animation Engine) - the same library used by
                3Blue1Brown. </p>
            </p>

            <p>
                Every time you use k-nearest neighbors, k-means clustering, or even search for similar products online,
                distance measures are working behind the scenes. But which distance metric should you use? The answer
                depends on your data's structure and the problem you're solving.
            </p>

            <h2>What is a Distance Measure?</h2>

            <p>
                A distance measure (or metric) quantifies how similar or different two data points are. Mathematically,
                a
                function \(d(x, y)\) is a valid distance metric if it satisfies four properties:
            </p>

            <ul>
                <li><strong>Non-negativity:</strong> \(d(x, y) \geq 0\)</li>
                <li><strong>Identity:</strong> \(d(x, y) = 0\) if and only if \(x = y\)</li>
                <li><strong>Symmetry:</strong> \(d(x, y) = d(y, x)\)</li>
                <li><strong>Triangle inequality:</strong> \(d(x, z) \leq d(x, y) + d(y, z)\)</li>
            </ul>

            <div class="animation-container">
                <div class="video-container">
                    <video id="euclidean-video" controls loop>
                        <source src="videos/distance/TriangleInequality.mp4" type="video/mp4">
                        Your browser doesn't support video playback.
                    </video>
                </div>
            </div>

            <h2>Euclidean Distance</h2>

            <p>
                The most intuitive distance measure is Euclidean distance—the straight-line distance between two points.
                This is what we commonly think of as "distance" in everyday life.
            </p>

            <div class="math-block">
                <p>\[d_{Euclidean}(x, y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}\]</p>
            </div>

            <p>
                In 2D space, this simplifies to the familiar Pythagorean theorem: \(d = \sqrt{(x_2-x_1)^2 +
                (y_2-y_1)^2}\)
            </p>

            <div class="animation-container">
                <div class="video-container">
                    <video id="euclidean-video" controls loop>
                        <source src="videos/distance/EuclideanDistance.mp4" type="video/mp4">
                        Your browser doesn't support video playback.
                    </video>
                </div>
            </div>

            <h3>When to Use Euclidean Distance</h3>

            <ul>
                <li>Data with continuous numerical features</li>
                <li>When the magnitude of differences matters</li>
                <li>Spatial data (coordinates, geographic locations)</li>
                <li>Image similarity (pixel-by-pixel comparison)</li>
            </ul>

            <div class="warning-box">
                <strong>Caution:</strong> Euclidean distance is sensitive to feature scale. Always normalize or
                standardize
                your features before using Euclidean distance, or features with larger ranges will dominate the
                calculation.
            </div>

            <h2>Manhattan Distance</h2>

            <p>
                Manhattan distance (also called L1 distance or Taxicab distance) measures the distance traveled along
                grid
                lines, like navigating city blocks in Manhattan.
            </p>

            <div class="math-block">
                <p>\[d_{Manhattan}(x, y) = \sum_{i=1}^{n}|x_i - y_i|\]</p>
            </div>

            <div class="animation-container">
                <div class="video-container">
                    <video id="manhattan-video" controls loop>
                        <source src="videos/distance/ManhattanDistance.mp4" type="video/mp4">
                        Your browser doesn't support video playback.
                    </video>
                </div>
            </div>

            <h3>Euclidean vs Manhattan: A Comparison</h3>

            <p>
                Let's see how these two metrics differ when measuring the same distance:
            </p>

            <div class="animation-container">
                <div class="video-container">
                    <video id="euclidean-manhattan-video" controls loop>
                        <source src="videos/distance/EuclideanManhattanDistance.mp4" type="video/mp4">
                        Your browser doesn't support video playback.
                    </video>
                </div>
            </div>

            <h3>When to Use Manhattan Distance</h3>

            <ul>
                <li>Grid-based problems (chess moves, robot navigation)</li>
                <li>When you want equal weight for each dimension</li>
                <li>High-dimensional spaces (less affected by the curse of dimensionality)</li>
                <li>More robust to outliers than Euclidean distance</li>
            </ul>

            <h2>Minkowski Distance</h2>

            <p>
                Minkowski distance is a generalization that includes both Euclidean and Manhattan distances as special
                cases:
            </p>

            <div class="math-block">
                <p>\[d_{Minkowski}(x, y) = \left(\sum_{i=1}^{n}|x_i - y_i|^p\right)^{1/p}\]</p>
            </div>

            <p>
                Where \(p\) is a parameter:
            </p>
            <ul>
                <li>\(p = 1\): Manhattan distance</li>
                <li>\(p = 2\): Euclidean distance</li>
                <li>\(p = \infty\): Chebyshev distance (maximum difference in any dimension)</li>
            </ul>

            <div class="animation-container">
                <div class="video-container">
                    <video id="minkowski-video" controls loop>
                        <source src="videos/distance/MinkowskiDistance.mp4" type="video/mp4">
                        Your browser doesn't support video playback.
                    </video>
                </div>
            </div>

            <h2>Cosine Similarity</h2>

            <p>
                Unlike the previous metrics, cosine similarity measures the angle between vectors rather than their
                magnitude. It's particularly useful when the direction matters more than the length.
            </p>

            <div class="math-block">
                <p>\[cosine\_similarity(x, y) = \frac{x \cdot y}{||x|| \cdot ||y||} = \frac{\sum_{i=1}^{n}x_i
                    y_i}{\sqrt{\sum_{i=1}^{n}x_i^2} \cdot \sqrt{\sum_{i=1}^{n}y_i^2}}\]</p>
            </div>

            <p>
                Cosine distance is then: \(d_{cosine} = 1 - cosine\_similarity\)
            </p>

            <div class="animation-container">
                <div class="video-container">
                    <video id="cosine-video" controls loop>
                        <source src="videos/distance/CosineSimilarity.mp4" type="video/mp4">
                        Your browser doesn't support video playback.
                    </video>
                </div>
            </div>

            <h3>When to Use Cosine Similarity</h3>

            <ul>
                <li>Text analysis and NLP (document similarity)</li>
                <li>Recommendation systems (user preference vectors)</li>
                <li>When magnitude is less important than direction</li>
                <li>High-dimensional sparse data (like TF-IDF vectors)</li>
            </ul>

            <div class="highlight-box">
                <strong>Key Insight:</strong> Two documents could have very different word counts but be considered
                similar
                if they discuss the same topics. Cosine similarity captures this by focusing on the pattern of words
                rather
                than their absolute frequencies.
            </div>

            <h2>Hamming Distance</h2>

            <p>
                Hamming distance measures the number of positions at which two strings or vectors differ. It's designed
                for
                categorical or binary data.
            </p>

            <div class="math-block">
                <p>\[d_{Hamming}(x, y) = \sum_{i=1}^{n} \mathbb{1}(x_i \neq y_i)\]</p>
            </div>

            <p>
                Where \(\mathbb{1}\) is the indicator function (1 if the condition is true, 0 otherwise).
            </p>

            <div class="animation-container">
                <div class="video-container">
                    <video id="hamming-video" controls loop>
                        <source src="videos/distance/HammingDistance.mp4" type="video/mp4">
                        Your browser doesn't support video playback.
                    </video>
                </div>
            </div>

            <h3>When to Use Hamming Distance</h3>

            <ul>
                <li>Binary strings or vectors</li>
                <li>Error detection and correction codes</li>
                <li>Categorical data (one-hot encoded features)</li>
                <li>DNA sequence comparison</li>
            </ul>

            <h2>Mahalanobis Distance</h2>

            <p>
                Mahalanobis distance accounts for correlations between variables and the variance of each variable. It's
                scale-invariant and considers the distribution of the data.
            </p>

            <div class="math-block">
                <p>\[d_{Mahalanobis}(x, y) = \sqrt{(x-y)^T S^{-1} (x-y)}\]</p>
            </div>

            <p>
                Where \(S\) is the covariance matrix of the data.
            </p>

            <div class="animation-container">
                <div class="video-container">
                    <video id="mahalanobis-video" controls loop>
                        <source src="videos/distance/MahalanobisDistance.mp4" type="video/mp4">
                        Your browser doesn't support video playback.
                    </video>
                </div>
            </div>

            <h3>When to Use Mahalanobis Distance</h3>

            <ul>
                <li>When features are correlated</li>
                <li>Anomaly detection (points far from the distribution center)</li>
                <li>Multivariate data with different scales</li>
                <li>When you need a scale-invariant distance</li>
            </ul>

            <div class="highlight-box">
                <strong>Key Advantage:</strong> Unlike Euclidean distance, Mahalanobis distance naturally handles
                features
                with different scales and accounts for correlations, making it more robust for real-world data.
            </div>

            <h2>Choosing the Right Distance Metric</h2>

            <p>
                The choice of distance metric can dramatically affect your algorithm's performance. Here's a practical
                guide:
            </p>

            <table>
                <thead>
                    <tr>
                        <th>Distance Metric</th>
                        <th>Best For</th>
                        <th>Limitations</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Euclidean</strong></td>
                        <td>Continuous features, spatial data</td>
                        <td>Sensitive to scale, assumes independence</td>
                    </tr>
                    <tr>
                        <td><strong>Manhattan</strong></td>
                        <td>Grid-like data, high dimensions</td>
                        <td>Ignores correlations</td>
                    </tr>
                    <tr>
                        <td><strong>Cosine</strong></td>
                        <td>Text, sparse data, direction matters</td>
                        <td>Ignores magnitude</td>
                    </tr>
                    <tr>
                        <td><strong>Hamming</strong></td>
                        <td>Categorical/binary data</td>
                        <td>Only for discrete data</td>
                    </tr>
                    <tr>
                        <td><strong>Mahalanobis</strong></td>
                        <td>Correlated features, anomaly detection</td>
                        <td>Requires covariance matrix, computationally expensive</td>
                    </tr>
                </tbody>
            </table>

            <h2>Real-World Applications</h2>

            <h3>K-Nearest Neighbors (KNN)</h3>

            <p>
                KNN classification relies entirely on distance measures to find the closest training examples to a new
                data
                point.
            </p>

            <div class="animation-container">
                <div class="video-container">
                    <video id=" k-means-video" controls loop>
                        <source src="videos/distance/KMeans.mp4" type="video/mp4">
                        Your browser doesn't support video playback.
                    </video>
                </div>
            </div>

            <h3>K-Means Clustering</h3>

            <p>
                K-means assigns points to clusters based on distance to cluster centroids. The choice of distance metric
                affects the cluster shapes.
            </p>


            <h2>The Curse of Dimensionality</h2>

            <p>
                As dimensions increase, distance measures behave differently. In high-dimensional spaces, all points
                tend to
                become equidistant, making distance-based algorithms less effective.
            </p>

            <div class="animation-container">
                <canvas id="dimensionalityCanvas" width="800" height="400"></canvas>
                <p class="caption">How distance distributions change as dimensionality increases.</p>
                <button class="control-button" onclick="animateDimensionality()">Replay Animation</button>
            </div>

            <div class="warning-box">
                <strong>Important:</strong> In high dimensions (>10 features), consider dimensionality reduction (PCA,
                t-SNE) before applying distance-based algorithms, or use Manhattan distance which is more robust to high
                dimensions.
            </div>

            <h2>Practical Tips</h2>

            <div class="comparison-grid">
                <div class="comparison-card">
                    <h4>Feature Scaling</h4>
                    <p>Always normalize or standardize features before using distance metrics (except Mahalanobis, which
                        is
                        scale-invariant).</p>
                </div>
                <div class="comparison-card">
                    <h4>Experiment</h4>
                    <p>Try multiple distance metrics with cross-validation to find what works best for your specific
                        problem.</p>
                </div>
                <div class="comparison-card">
                    <h4>Domain Knowledge</h4>
                    <p>Consider what "similarity" means in your domain. For text, cosine is often better than Euclidean.
                    </p>
                </div>
                <div class="comparison-card">
                    <h4>Computational Cost</h4>
                    <p>Manhattan is faster than Euclidean (no square root). Mahalanobis is most expensive.</p>
                </div>
            </div>

            <h2>Conclusion</h2>

            <p>
                Distance measures are fundamental building blocks in machine learning. The right choice depends on your
                data
                type, dimensionality, and what "similarity" means in your problem domain.
            </p>

            <p>
                Key takeaways:
            </p>

            <ul>
                <li>Euclidean distance is intuitive but requires feature scaling</li>
                <li>Manhattan distance is more robust in high dimensions</li>
                <li>Cosine similarity focuses on direction, not magnitude</li>
                <li>Mahalanobis distance accounts for correlations and scale</li>
                <li>Always experiment with multiple metrics</li>
            </ul>

            <div class="highlight-box">
                <strong>Next Steps:</strong> Explore custom distance metrics tailored to your specific problem. Many
                libraries (scikit-learn, scipy) allow you to define your own distance functions for specialized
                applications.
            </div>

            <h2>References</h2>
            <ul style="font-size: 0.9em; color: #4a5568;">
                <li>[1] Bishop, C. M. (2006). <a href="#">Pattern Recognition and Machine Learning</a>. Springer.</li>
                <li>[2] Deza, M. M., & Deza, E. (2009). <a href="#">Encyclopedia of Distances</a>. Springer.</li>
                <li>[3] Sanderson, G. (3Blue1Brown). <a href="#">Essence of Linear Algebra</a>. YouTube Series.</li>
            </ul>

            <div
                style="margin-top: 60px; padding-top: 30px; border-top: 1px solid #e2e8f0; color: #718096; text-align: center;">
                <p>Thank you for reading! I hope these visualizations clarified how different distance measures work.
                </p>
                <p>Understanding these fundamentals will help you make better choices in clustering, classification, and
                    beyond.</p>
            </div>
        </div>
    </div>
</body>

</html>