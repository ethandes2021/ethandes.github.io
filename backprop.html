<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Visual Guide to Backpropagation</title>
    <link rel="stylesheet" href="css/style.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <div class="container">
        <a href="index.html" class="back-button">‚Üê Back to Blog</a>
        <div class="article-header-row">
            <div class="article-header-text">
                <h1>A Visual Guide to Backpropagation</h1>
            </div>
            <img src="assets/images/backprop-banner.png" alt="Backpropagation Banner" class="article-banner-side">
        </div>

        <p class="intro">
            Neural networks learn through a process called backpropagation. But what exactly happens when we
            "backpropagate" through a network? In this visual guide, we'll explore this fundamental algorithm through
            interactive animations that reveal how neural networks adjust their weights to minimize error.
        </p>
        <p>The animations are created with Manim (Mathematical Animation Engine) - the same library used by
            3Blue1Brown. </p>

        <p>
            Backpropagation is the cornerstone of modern deep learning. Despite its importance, the algorithm often
            feels like a black box. Through more than 15 visualizations, we'll build an intuition for how gradients flow
            backward through a network, updating weights to improve predictions.
        </p>

        <h2>What is Backpropagation?</h2>

        <p>
            Backpropagation, short for "backward propagation of errors," is an algorithm for training neural networks.
            It efficiently computes the gradient of the loss function with respect to each weight in the network.
        </p>

        <p>
            Two main phases define the learning process:
        </p>

        <ul>
            <li><strong>Forward Pass</strong> - Input data flows through the network, layer by layer, producing a
                prediction.</li>
            <li><strong>Backward Pass</strong> - The error flows backward through the network, computing gradients and
                updating weights.</li>
        </ul>

        <div class="animation-container">
            <div class="video-container">
                <video id="forwardPass-video" controls loop>
                    <source src="videos/backprop/ForwardPass.mp4" type="video/mp4">
                    Your browser doesn't support video playback.
                </video>
            </div>
        </div>

        <h2>The Forward Pass</h2>

        <p>
            Before we can backpropagate, we need to understand what happens during the forward pass. Let's visualize how
            information flows through a simple neural network.
        </p>

        <h3>Computing Activations</h3>

        <p>
            At each layer, we compute a weighted sum of inputs plus a bias term, then apply an activation function:
        </p>

        <div class="math-block">
            <p>\[z^{[l]} = W^{[l]} \cdot a^{[l-1]} + b^{[l]}\]</p>
            <p>\[a^{[l]} = \sigma(z^{[l]})\]</p>
        </div>

        <p>
            Where \(\sigma\) is the activation function (like ReLU or sigmoid), \(W^{[l]}\) are the weights, and
            \(b^{[l]}\) is the bias for layer \(l\).
        </p>

        <div class="animation-container">
            <div class="video-container">
                <video id="neuronComputation-video" controls loop>
                    <source src="videos/backprop/NeuronComputation.mp4" type="video/mp4">
                    Your browser doesn't support video playback.
                </video>
            </div>
        </div>

        <h3>The Loss Function</h3>

        <p>
            After the forward pass completes, we compare our prediction with the actual target value using a loss
            function. For regression, we might use Mean Squared Error (MSE):
        </p>

        <div class="math-block">
            <p>\[L = \frac{1}{2}(y_{pred} - y_{true})^2\]</p>
        </div>

        <p>
            This loss tells us how wrong our network is. The goal of backpropagation is to adjust the weights to
            minimize this loss.
        </p>

        <h2>The Backward Pass</h2>

        <p>
            Now comes the magic. Backpropagation uses the chain rule from calculus to efficiently compute how much each
            weight contributed to the error. These gradients tell us which direction to adjust each weight.
        </p>

        <h3>The Chain Rule</h3>

        <p>
            The chain rule is the mathematical foundation of backpropagation. It allows us to compute derivatives of
            composite functions:
        </p>

        <div class="math-block">
            <p>\[\frac{\partial L}{\partial w} = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot
                \frac{\partial z}{\partial w}\]</p>
        </div>

        <p>
            Let's break this down visually. Each term represents how sensitive one quantity is to changes in another.
        </p>

        <div class="animation-container">
            <div class="video-container">
                <video id="chainRule-video" controls loop>
                    <source src="videos/backprop/ChainRule.mp4" type="video/mp4">
                    Your browser doesn't support video playback.
                </video>
            </div>
        </div>

        <h3>Computing Gradients Layer by Layer</h3>

        <p>
            Backpropagation works by computing gradients starting from the output layer and moving backward through the
            network. At each layer, we compute:
        </p>

        <div class="math-block">
            <p>\[\frac{\partial L}{\partial W^{[l]}} = \frac{\partial L}{\partial z^{[l]}} \cdot (a^{[l-1]})^T\]</p>
            <p>\[\frac{\partial L}{\partial b^{[l]}} = \frac{\partial L}{\partial z^{[l]}}\]</p>
            <p>\[\frac{\partial L}{\partial a^{[l-1]}} = (W^{[l]})^T \cdot \frac{\partial L}{\partial z^{[l]}}\]</p>
        </div>

        <div class="animation-container">
            <div class="video-container">
                <video id="backprop-video" controls loop>
                    <source src="videos/backprop/BackpropagationFlow.mp4" type="video/mp4">
                    Your browser doesn't support video playback.
                </video>
            </div>
        </div>

        <h2>Gradient Descent: Updating Weights</h2>

        <p>
            Once we've computed the gradients, we update the weights using gradient descent:
        </p>

        <div class="math-block">
            <p>\[W^{[l]} = W^{[l]} - \alpha \frac{\partial L}{\partial W^{[l]}}\]</p>
        </div>

        <p>
            Where \(\alpha\) is the learning rate, a hyperparameter that controls how big our update steps are.
        </p>

        <div class="animation-container">
            <div class="video-container">
                <video id="gradient-video" controls loop>
                    <source src="videos/backprop/GradientDescent.mp4" type="video/mp4">
                    Your browser doesn't support video playback.
                </video>
            </div>
        </div>

        <h3>The Learning Rate</h3>

        <p>
            The learning rate is crucial. Too large, and we might overshoot the minimum. Too small, and training will be
            painfully slow.
        </p>

        <div class="highlight-box">
            <strong>Key Insight:</strong> The learning rate determines how aggressively we update weights. Finding the
            right learning rate is often critical for successful training.
        </div>

        <h2>A Complete Example</h2>

        <p>
            Let's put everything together and watch a complete training iteration: forward pass, loss computation,
            backward pass, and weight update.
        </p>

        <div class="animation-container">
            <div class="video-container">
                <video id="complete-video" controls loop>
                    <source src="videos/backprop/CompleteTrainingCycle.mp4" type="video/mp4">
                    Your browser doesn't support video playback.
                </video>
            </div>
        </div>

        <h2>Why Backpropagation Matters</h2>

        <p>
            Before backpropagation was popularized in the 1980s, training neural networks was extremely difficult.
            Backpropagation made it possible to train deep networks efficiently by:
        </p>

        <ul>
            <li>Computing gradients for millions of parameters in reasonable time</li>
            <li>Enabling automatic differentiation frameworks like PyTorch and TensorFlow</li>
            <li>Making deep learning practical and scalable</li>
        </ul>

        <div class="highlight-box">
            <strong>Historical Note:</strong> While the basic idea of backpropagation dates back to the 1960s, it wasn't
            widely adopted until Rumelhart, Hinton, and Williams popularized it in their 1986 paper "Learning
            representations by back-propagating errors."
        </div>

        <h2>Common Challenges</h2>

        <h3>Vanishing Gradients</h3>

        <p>
            In deep networks, gradients can become extremely small as they propagate backward, effectively preventing
            early layers from learning. This is called the vanishing gradient problem.
        </p>

        <div class="animation-container">
            <div class="video-container">
                <video id="vanishingGradient-video" controls loop>
                    <source src="videos/backprop/VanishingGradient.mp4" type="video/mp4">
                    Your browser doesn't support video playback.
                </video>
            </div>
        </div>

        <h3>Exploding Gradients</h3>

        <p>
            Conversely, gradients can also grow exponentially large, causing unstable training. This is the exploding
            gradient problem.
        </p>

        <p>
            Modern solutions include:
        </p>
        <ul>
            <li>Using ReLU activation functions instead of sigmoid</li>
            <li>Batch normalization to stabilize gradients</li>
            <li>Gradient clipping to prevent explosions</li>
            <li>Residual connections (skip connections) in architectures like ResNet</li>
        </ul>

        <h2>Beyond Basic Backpropagation</h2>

        <p>
            Modern deep learning has extended backpropagation in several ways:
        </p>

        <ul>
            <li><strong>Momentum</strong> - Accumulates gradients over time to accelerate convergence</li>
            <li><strong>Adam Optimizer</strong> - Adapts learning rates for each parameter individually</li>
            <li><strong>Mini-batch Training</strong> - Computes gradients over small batches for efficiency</li>
            <li><strong>Automatic Differentiation</strong> - Frameworks automatically compute gradients for any
                computation graph</li>
        </ul>

        <h2>Conclusion</h2>

        <p>
            Backpropagation is the engine that powers modern deep learning. By efficiently computing gradients through
            the chain rule, it enables networks with millions or billions of parameters to learn from data.
        </p>

        <p>
            Understanding backpropagation deeply provides insight into:
        </p>

        <ul>
            <li>Why certain architectures work better than others</li>
            <li>How to debug training problems</li>
            <li>How to design custom layers and loss functions</li>
            <li>The fundamentals underlying all neural network training</li>
        </ul>

        <div class="highlight-box">
            <strong>Next Steps:</strong> Now that you understand backpropagation, explore how it applies to
            convolutional layers, recurrent networks, and attention mechanisms. Each architecture requires adapting the
            core backpropagation algorithm to its specific structure.
        </div>

        <h2>References</h2>
        <ul style="font-size: 0.9em; color: #4a5568;">
            <li>[1] Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). <a href="#">Learning representations by
                    back-propagating errors</a>. Nature.</li>
            <li>[2] Nielsen, M. (2015). <a href="#">Neural Networks and Deep Learning</a>. Determination Press.</li>
            <li>[3] Sanderson, G. (3Blue1Brown). <a href="#">Neural Networks</a>. YouTube Series.</li>
            <li>[4] Goodfellow, I., Bengio, Y., & Courville, A. (2016). <a href="#">Deep Learning</a>. MIT Press.</li>
        </ul>

        <div class="footer">
            <p>Thank you for reading! I hope these visualizations helped demystify backpropagation.</p>
            <p>Feel free to share this guide with anyone learning about neural networks.</p>
        </div>
    </div>
</body>

</html>